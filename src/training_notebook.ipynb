{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `train_model.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file processes the training data and trains the model accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import MediaPipe hands**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1738639552.427457 28918797 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1738639552.450677 28920900 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738639552.466601 28920898 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "\n",
    "# MediaPipe hands & extract_keypoints from main.py\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=True, \n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Extract keypoints**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This `extract_keypoints` function: \n",
    "1) Resizes each image\n",
    "2) Changes image to RGB (Red, Green, Blue) for MediaPipe because OpenCv is in BGR\n",
    "3) Gets landmarks for each image (INSERT MEDIAPIPE LANDMARK IMAGE HERE)\n",
    "4) Expected output is a list of 63 values (21 landmarks x 3 coordinates) for each image/hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "def extract_keypoints(image):\n",
    "    \"\"\" Extract hand keypoints from user using MediaPipe \"\"\"\n",
    "    try:\n",
    "        # Revise image -> better preformance\n",
    "        image = cv.resize(image, (640, 480)) # (640, 480) is default webcam resolution\n",
    "                                             # Consistent input size will help improve speed \n",
    "                                             # and stability of landmark detection\n",
    "\n",
    "        # Convert image to RGB spectrum\n",
    "        image_rgb = cv.cvtColor(image, cv.COLOR_BGR2RGB) # OpenCV images are by default BGR format\n",
    "        results = hands.process(image_rgb) # Process image with MediaPipe\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            landmarks = results.multi_hand_landmarks[0].landmark # Get hand's landmark\n",
    "                                                                 # returns a list of 21 landmark points for a single hand detected\n",
    "                                                                 # Each landmark has an x, y, and z coordinate\n",
    "            keypoints = []\n",
    "            for landmark in landmarks:\n",
    "                keypoints.append(landmark.x)  # x-coord\n",
    "                keypoints.append(landmark.y)  # Y-coord\n",
    "                keypoints.append(landmark.z)  # Z-coord (depth)\n",
    "            return keypoints\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\")\n",
    "    return None\n",
    "\n",
    "    # Expected output is a list of 63 values (21 landmarks x 3 coordinates) for an image/hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prepare data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This `prepare_data` function:\n",
    "1) Goes through each class (letter) folder and processes each of the 3000 photos\n",
    "2) Applies extract_keypoints to each image\n",
    "3) It then takes each list of 63 values (21 landmarks x 3 coordinates) for each image/hand and saves them as numpy arrays for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def prepare_data(data_dir, num_samples=None):\n",
    "    \"\"\" Prepare the training data \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    sample_count = 0\n",
    "\n",
    "    # Iterate through each class/letter folder\n",
    "    for label in os.listdir(data_dir):\n",
    "        class_folder = os.path.join(data_dir, label)\n",
    "        if os.path.isdir(class_folder):\n",
    "            class_images = os.listdir(class_folder)\n",
    "            print(f\"Processing {len(class_images)} images for class {label}\")\n",
    "\n",
    "            # Process each image in the class folder\n",
    "            for img_name in class_images:\n",
    "                img_path = os.path.join(class_folder, img_name)\n",
    "                image = cv.imread(img_path) # Reads the image with OpenCV\n",
    "                \n",
    "                if image is not None:\n",
    "                    keypoints = extract_keypoints(image)  # MediaPipe landmarks applied to image if hand detected \n",
    "                    if keypoints:\n",
    "                        images.append(keypoints) \n",
    "                        labels.append(label)\n",
    "                        sample_count += 1\n",
    "                        \n",
    "                        # Optionally stop after 'num_samples' images for quick testing\n",
    "                        if num_samples and sample_count >= num_samples:\n",
    "                            break\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    print(f\"Total images loaded: {len(images)}\")\n",
    "\n",
    "    # Encode labels as integers\n",
    "    label_encoder = LabelEncoder() # sklearn\n",
    "    labels = label_encoder.fit_transform(labels)\n",
    "    \n",
    "    return images, labels, label_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prepare the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset, saving X, y, label_encoder for each image after processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3000 images for class R\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Prepare the data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/carlopisacane/Desktop/Honors/HonorsThesis/data/asl_alphabet_train\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Path to your dataset\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m X, y, label_encoder \u001b[38;5;241m=\u001b[39m prepare_data(data_dir)\n",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m, in \u001b[0;36mprepare_data\u001b[0;34m(data_dir, num_samples)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_name \u001b[38;5;129;01min\u001b[39;00m class_images:\n\u001b[1;32m     20\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(class_folder, img_name)\n\u001b[0;32m---> 21\u001b[0m     image \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mimread(img_path) \u001b[38;5;66;03m# Reads the image with OpenCV\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m         keypoints \u001b[38;5;241m=\u001b[39m extract_keypoints(image)  \u001b[38;5;66;03m# MediaPipe landmarks applied to image if hand detected \u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Prepare the data\n",
    "data_dir = '/Users/carlopisacane/Desktop/Honors/HonorsThesis/data/asl_alphabet_train'  # Path to your dataset\n",
    "X, y, label_encoder = prepare_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This model is trained by:\n",
    "1) Split the data into training and validation sets\n",
    "2) Build a machine learning model with two **Dense** layers and **Dropout** for regularization\n",
    "3) Train the model on the extracted keypoints for 10 epochs\n",
    "4) Save the trained model to a file named `asl_model.h5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Split data into training and validation sets\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Training set (X_train, y_train)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Validation set (X_val, y_val)  \u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining samples:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Training set (X_train, y_train)\n",
    "# Validation set (X_val, y_val)  \n",
    "print(\"Training samples:\", X_train.shape[0])\n",
    "print(\"Validation samples:\", X_val.shape[0])\n",
    "\n",
    "# test_size=0.2 means 20% of data goes to validation and 80% remains for training model\n",
    "# random_state=42 ensures that the split is reproducible\n",
    "\n",
    "# You need validation because the model is always adjusting weights to minimize loss,\n",
    "# you risk overfitting the model if only track performance on data being trained.\n",
    "\n",
    "# In model.fit -> the validation data is set at the end of each epoch and reports the validation\n",
    "# loss and accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Dropout\u001b[38;5;66;03m#, Flaten\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Build the model with regularization\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[0;32m----> 6\u001b[0m     Dense(\u001b[38;5;241m128\u001b[39m, input_dim\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;66;03m# Input layer and first dense layer\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     Dropout(\u001b[38;5;241m0.5\u001b[39m), \u001b[38;5;66;03m# Sets 50% of inputs to zero, help prevent overfitting\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     Dense(\u001b[38;5;241m64\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;66;03m# Second dense layer\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     Dropout(\u001b[38;5;241m0.5\u001b[39m), \u001b[38;5;66;03m# Another dropout layer\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     Dense(\u001b[38;5;241m26\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# Softmax outputs probability distribution across all 26 classes\u001b[39;00m\n\u001b[1;32m     11\u001b[0m ])\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# Adapts the learning rate for each parameter using Adam\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     17\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     18\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout#, Flaten\n",
    "\n",
    "# Build the model with regularization\n",
    "model = Sequential([\n",
    "    Dense(128, input_dim=X_train.shape[1], activation='relu'), # Input layer and first dense layer\n",
    "    Dropout(0.5), # Sets 50% of inputs to zero, help prevent overfitting\n",
    "    Dense(64, activation='relu'), # Second dense layer\n",
    "    Dropout(0.5), # Another dropout layer\n",
    "    Dense(26, activation='softmax') # Softmax outputs probability distribution across all 26 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', # Adapts the learning rate for each parameter using Adam\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary() # Print model architecture summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and save it in `asl_model.h5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m      3\u001b[0m     X_train, y_train, \n\u001b[1;32m      4\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \n\u001b[1;32m      5\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val)\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel/asl_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved as asl_model.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=10, \n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "\n",
    "model.save('model/asl_model.h5')  # Save the trained model\n",
    "print(\"Model saved as asl_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
