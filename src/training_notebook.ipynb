{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `train_model.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file processes the training data and trains the model accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import MediaPipe hands**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1739418070.823797 1860667 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1739418070.834967 1860895 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1739418070.841658 1860895 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "\n",
    "# MediaPipe hands & extract_keypoints from main.py\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=True, \n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Extract keypoints**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This `extract_keypoints` function: \n",
    "1) Resizes each image\n",
    "2) Changes image to RGB (Red, Green, Blue) for MediaPipe because OpenCv is in BGR\n",
    "3) Gets landmarks for each image (INSERT MEDIAPIPE LANDMARK IMAGE HERE)\n",
    "4) Expected output is a list of 63 values (21 landmarks x 3 coordinates) for each image/hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "def extract_keypoints(image):\n",
    "    \"\"\" Extract hand keypoints from user using MediaPipe \"\"\"\n",
    "    try:\n",
    "        # Revise image -> better preformance\n",
    "        image = cv.resize(image, (640, 480)) # (640, 480) is default webcam resolution\n",
    "                                             # Consistent input size will help improve speed \n",
    "                                             # and stability of landmark detection\n",
    "\n",
    "        # Convert image to RGB spectrum\n",
    "        image_rgb = cv.cvtColor(image, cv.COLOR_BGR2RGB) # OpenCV images are by default BGR format\n",
    "        results = hands.process(image_rgb) # Process image with MediaPipe\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            landmarks = results.multi_hand_landmarks[0].landmark # Get hand's landmark\n",
    "                                                                 # returns a list of 21 landmark points for a single hand detected\n",
    "                                                                 # Each landmark has an x, y, and z coordinate\n",
    "            keypoints = []\n",
    "            for landmark in landmarks:\n",
    "                keypoints.append(landmark.x)  # x-coord\n",
    "                keypoints.append(landmark.y)  # Y-coord\n",
    "                keypoints.append(landmark.z)  # Z-coord (depth)\n",
    "            return keypoints\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\")\n",
    "    return None\n",
    "\n",
    "    # Expected output is a list of 63 values (21 landmarks x 3 coordinates) for an image/hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prepare data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This `prepare_data` function:\n",
    "1) Goes through each class (letter) folder and processes each of the 3000 photos\n",
    "2) Applies extract_keypoints to each image\n",
    "3) It then takes each list of 63 values (21 landmarks x 3 coordinates) for each image/hand and saves them as numpy arrays for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def prepare_data(data_dir, num_samples=None):\n",
    "    \"\"\" Prepare the training data \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    sample_count = 0\n",
    "\n",
    "    # Iterate through each class/letter folder\n",
    "    for label in os.listdir(data_dir):\n",
    "        class_folder = os.path.join(data_dir, label)\n",
    "        if os.path.isdir(class_folder):\n",
    "            class_images = os.listdir(class_folder)\n",
    "            print(f\"Processing {len(class_images)} images for class {label}\")\n",
    "\n",
    "            # Process each image in the class folder\n",
    "            for img_name in class_images:\n",
    "                img_path = os.path.join(class_folder, img_name)\n",
    "                image = cv.imread(img_path) # Reads the image with OpenCV\n",
    "                \n",
    "                if image is not None:\n",
    "                    keypoints = extract_keypoints(image)  # MediaPipe landmarks applied to image if hand detected \n",
    "                    if keypoints:\n",
    "                        images.append(keypoints) \n",
    "                        labels.append(label)\n",
    "                        sample_count += 1\n",
    "                        \n",
    "                        # Optionally stop after 'num_samples' images for quick testing\n",
    "                        if num_samples and sample_count >= num_samples:\n",
    "                            break\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    print(f\"Total images loaded: {len(images)}\")\n",
    "\n",
    "    # Encode labels as integers\n",
    "    label_encoder = LabelEncoder() # sklearn\n",
    "    labels = label_encoder.fit_transform(labels)\n",
    "    \n",
    "    return images, labels, label_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prepare the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset, saving X, y, label_encoder for each image after processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3000 images for class R\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1739417312.815430 1823255 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Prepare the data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/carlopisacane/Desktop/Honors/HonorsThesis/data/asl_alphabet_train\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Path to your dataset\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m X, y, label_encoder \u001b[38;5;241m=\u001b[39m prepare_data(data_dir)\n",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m, in \u001b[0;36mprepare_data\u001b[0;34m(data_dir, num_samples)\u001b[0m\n\u001b[1;32m     21\u001b[0m image \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mimread(img_path) \u001b[38;5;66;03m# Reads the image with OpenCV\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m     keypoints \u001b[38;5;241m=\u001b[39m extract_keypoints(image)  \u001b[38;5;66;03m# MediaPipe landmarks applied to image if hand detected \u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m keypoints:\n\u001b[1;32m     26\u001b[0m         images\u001b[38;5;241m.\u001b[39mappend(keypoints) \n",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m, in \u001b[0;36mextract_keypoints\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m      8\u001b[0m                                      \u001b[38;5;66;03m# Consistent input size will help improve speed \u001b[39;00m\n\u001b[1;32m      9\u001b[0m                                      \u001b[38;5;66;03m# and stability of landmark detection\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Convert image to RGB spectrum\u001b[39;00m\n\u001b[1;32m     12\u001b[0m image_rgb \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mcvtColor(image, cv\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB) \u001b[38;5;66;03m# OpenCV images are by default BGR format\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m results \u001b[38;5;241m=\u001b[39m hands\u001b[38;5;241m.\u001b[39mprocess(image_rgb) \u001b[38;5;66;03m# Process image with MediaPipe\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks:\n\u001b[1;32m     16\u001b[0m     landmarks \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlandmark \u001b[38;5;66;03m# Get hand's landmark\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/mediapipe/python/solutions/hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[1;32m    133\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mprocess(input_data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: image})\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/mediapipe/python/solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[1;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[1;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[0;32m--> 340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39mwait_until_idle()\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Prepare the data\n",
    "data_dir = '/Users/carlopisacane/Desktop/Honors/HonorsThesis/data/asl_alphabet_train'  # Path to your dataset\n",
    "X, y, label_encoder = prepare_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This model is trained by:\n",
    "1) Split the data into training and validation sets\n",
    "2) Build a machine learning model with two **Dense** layers and **Dropout** for regularization\n",
    "3) Train the model on the extracted keypoints for 10 epochs\n",
    "4) Save the trained model to a file named `asl_model.h5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Split data into training and validation sets\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Training set (X_train, y_train)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Validation set (X_val, y_val)  \u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining samples:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Training set (X_train, y_train)\n",
    "# Validation set (X_val, y_val)  \n",
    "print(\"Training samples:\", X_train.shape[0])\n",
    "print(\"Validation samples:\", X_val.shape[0])\n",
    "\n",
    "# test_size=0.2 means 20% of data goes to validation and 80% remains for training model\n",
    "# random_state=42 ensures that the split is reproducible\n",
    "\n",
    "# You need validation because the model is always adjusting weights to minimize loss,\n",
    "# you risk overfitting the model if only track performance on data being trained.\n",
    "\n",
    "# In model.fit -> the validation data is set at the end of each epoch and reports the validation\n",
    "# loss and accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Dropout\u001b[38;5;66;03m#, Flaten\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Build the model with regularization\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[0;32m----> 6\u001b[0m     Dense(\u001b[38;5;241m128\u001b[39m, input_dim\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;66;03m# Input layer and first dense layer\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     Dropout(\u001b[38;5;241m0.5\u001b[39m), \u001b[38;5;66;03m# Sets 50% of inputs to zero, help prevent overfitting\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     Dense(\u001b[38;5;241m64\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;66;03m# Second dense layer\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     Dropout(\u001b[38;5;241m0.5\u001b[39m), \u001b[38;5;66;03m# Another dropout layer\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     Dense(\u001b[38;5;241m26\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# Softmax outputs probability distribution across all 26 classes\u001b[39;00m\n\u001b[1;32m     11\u001b[0m ])\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# Adapts the learning rate for each parameter using Adam\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     17\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     18\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout#, Flaten\n",
    "\n",
    "# Build the model with regularization\n",
    "model = Sequential([\n",
    "    Dense(128, input_dim=X_train.shape[1], activation='relu'), # Input layer and first dense layer\n",
    "    Dropout(0.5), # Sets 50% of inputs to zero, help prevent overfitting\n",
    "    Dense(64, activation='relu'), # Second dense layer\n",
    "    Dropout(0.5), # Another dropout layer\n",
    "    Dense(26, activation='softmax') # Softmax outputs probability distribution across all 26 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', # Adapts the learning rate for each parameter using Adam\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary() # Print model architecture summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and save it in `asl_model.h5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m      3\u001b[0m     X_train, y_train, \n\u001b[1;32m      4\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \n\u001b[1;32m      5\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val)\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel/asl_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved as asl_model.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=10, \n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "\n",
    "model.save('model/asl_model.h5')  # Save the trained model\n",
    "print(\"Model saved as asl_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
