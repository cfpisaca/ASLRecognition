{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `main.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Initialize video capture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 22:41:20.975 python[17320:1008420] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import tensorflow as tf \n",
    "\n",
    "cap = cv.VideoCapture(0) # Webcam capture \n",
    "                         # cap is used later to read frames in a loop\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import MediaPipe hands, initialize FPS calculation, load trained model `asl_model.h5`, and map letters/classes** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1739418084.003343 1008420 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1739418084.012337 1861185 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1739418084.020041 1861183 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "from cvfpscalc import CvFpsCalc \n",
    "\n",
    "# MediaPipe documentation\n",
    "# Google      # https://ai.google.dev/edge/mediapipe/solutions/vision/hand_landmarker\n",
    "# readthedocs # https://mediapipe.readthedocs.io/en/latest/solutions/hands.html \n",
    "    \n",
    "# Initialize MediaPipe hands model    \n",
    "mp_hands = mp.solutions.hands                       \n",
    "hands = mp_hands.Hands( \n",
    "    static_image_mode=False,  \n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.7, # Higher confidence is worse in poor lightning\n",
    "    min_tracking_confidence=0.6\n",
    ")\n",
    "\n",
    "# Initialize FPS calculation\n",
    "fps_calc = CvFpsCalc() # Imported over from another project on GitHub -> cvfpscalc.py\n",
    "\n",
    "# Load the trained model ('asl_model.h5')\n",
    "model = tf.keras.models.load_model('model/asl_model.h5')\n",
    "\n",
    "# Label mapping (letters A-Z)\n",
    "class_labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'no_gesture']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Extract keypoints**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This `extract_keypoints` function: \n",
    "1) Resizes each image\n",
    "2) Changes image to RGB (Red, Green, Blue) for MediaPipe because OpenCv is in BGR\n",
    "3) Gets landmarks for each image (INSERT MEDIAPIPE LANDMARK IMAGE HERE)\n",
    "4) Expected output is a list of 63 values (21 landmarks x 3 coordinates) for each image/hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(image):\n",
    "    \"\"\" Extract hand keypoints from user using MediaPipe \"\"\"\n",
    "    try:\n",
    "        # Revise image -> better preformance\n",
    "        image = cv.resize(image, (640, 480)) # (640, 480) is default webcam resolution\n",
    "                                             # Consistent input size will help improve speed \n",
    "                                             # and stability of landmark detection\n",
    "\n",
    "        # Convert image to RGB spectrum\n",
    "        image_rgb = cv.cvtColor(image, cv.COLOR_BGR2RGB) # OpenCV images are by default BGR format\n",
    "        results = hands.process(image_rgb) # Process image with MediaPipe\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            landmarks = results.multi_hand_landmarks[0].landmark # Get hand's landmark\n",
    "                                                                 # returns a list of 21 landmark points for a single hand detected\n",
    "                                                                 # Each landmark has an x, y, and z coordinate\n",
    "            keypoints = []\n",
    "            for landmark in landmarks:\n",
    "                keypoints.append(landmark.x)  # x-coord\n",
    "                keypoints.append(landmark.y)  # Y-coord\n",
    "                keypoints.append(landmark.z)  # Z-coord (depth)\n",
    "            return keypoints\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\")\n",
    "    return None\n",
    "\n",
    "    # Expected output is a list of 63 values (21 landmarks x 3 coordinates) for an image/hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Classify gesture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This `classify_gesture` function:\n",
    "1) Takes keypoints (63) and converts to numpy array to fit the model input\n",
    "2) Flattens into a 1D array\n",
    "3) Predicts the model\n",
    "4) Returns the label of predicted letter/class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def classify_gesture(keypoints):\n",
    "    \"\"\" Classify the hand gesture using the trained model \"\"\"\n",
    "    keypoints = np.array([keypoints])  # Convert keypoints to numpy array to fit the model input\n",
    "    keypoints = keypoints.reshape(1, -1)  # Flatten the keypoints to a 1D array for the model input\n",
    "    prediction = model.predict(keypoints) # Model prediction\n",
    "    predicted_class = np.argmax(prediction) # Find the class with with highest prob.\n",
    "    return class_labels[predicted_class] # Return label mapping of predicted class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Main Loop for Hand Gesture Recognition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `while` loop for each frame:\n",
    "1) Flips image for mirror view -> easier for user to sign\n",
    "2) Converts to RBG for MediaPipe and OpenCV processing\n",
    "3) Calculates FPS\n",
    "4) If hands are detected `multi_hand_landmarks` is a list of landmark sets (21) for each hand detected, then...\n",
    "5) For `hand_landmarks` in `results.multi_hand_landmarks` you have to...\n",
    "6) Iterate over each hand detected, draw landmarks, create a bounding box, then...\n",
    "7) Get landmarks for classification (`classify_gesture`), draw skeleton and put detected letters above the bounding box\n",
    "8) Display FPS\n",
    "9) \"ECS\" feature\n",
    "10) Then finally, release cam and close window when code is done and executed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 22:41:31.899 python[17320:1008420] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2025-02-12 22:41:31.899 python[17320:1008420] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     ret, image \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread() \u001b[38;5;66;03m# One frame\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Failed to capture image.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    ret, image = cap.read() # One frame\n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture image.\")\n",
    "        break\n",
    "\n",
    "    image = cv.flip(image, 1)  # Flip the image for mirror view -> for some reason works better for a-b-c-d-e-f (on left hand)\n",
    "\n",
    "    # Convert the image to RGB for MediaPipe processing\n",
    "    image_rgb = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "    image_rgb.flags.writeable = False\n",
    "    results = hands.process(image_rgb) \n",
    "    image_rgb.flags.writeable = True\n",
    "\n",
    "    # Calculate FPS\n",
    "    fps = fps_calc.get()\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Draw the landmarks on the image\n",
    "            h, w, _ = image.shape # _ Represents color channels (RGB), but we don't need it so _ is an unused variable, system crashes without it \n",
    "            hand_landmarks_list = []\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                cx, cy = int(landmark.x * w), int(landmark.y * h)\n",
    "                hand_landmarks_list.append((cx, cy))  # Store landmarks coordinates\n",
    "                cv.circle(image, (cx, cy), 5, (0, 255, 0), -1)  # Draw the landmark dots\n",
    "\n",
    "            # Create a bounding box around the hand\n",
    "            min_x = min(hand_landmarks_list, key=lambda item: item[0])[0]\n",
    "            max_x = max(hand_landmarks_list, key=lambda item: item[0])[0]\n",
    "            min_y = min(hand_landmarks_list, key=lambda item: item[1])[1]\n",
    "            max_y = max(hand_landmarks_list, key=lambda item: item[1])[1]\n",
    "\n",
    "            # Draw the bounding box \n",
    "            cv.rectangle(image, (min_x, min_y), (max_x, max_y), (0, 255, 0), 2)  \n",
    "\n",
    "            # Get the landmarks for classification\n",
    "            landmark_list = [(landmark.x, landmark.y, landmark.z) for landmark in hand_landmarks.landmark]\n",
    "            detected_letter = classify_gesture(landmark_list)\n",
    "\n",
    "            # Draw the hand skeleton\n",
    "            for connection in mp_hands.HAND_CONNECTIONS:\n",
    "                start = hand_landmarks.landmark[connection[0]]\n",
    "                end = hand_landmarks.landmark[connection[1]]\n",
    "                start_point = int(start.x * w), int(start.y * h)\n",
    "                end_point = int(end.x * w), int(end.y * h)\n",
    "                cv.line(image, start_point, end_point, (0, 0, 255), 2)\n",
    "\n",
    "            # Display the detected letter above the bounding box\n",
    "            if detected_letter != \"no_gesture\":\n",
    "                cv.putText(image, f'{detected_letter}', (min_x, min_y - 10), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Display FPS\n",
    "    cv.putText(image, f'FPS: {int(fps)}', (10, 30), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the processed image\n",
    "    cv.imshow('Hand Gesture Recognition', image)\n",
    "\n",
    "    # Exit when 'ESC' is pressed\n",
    "    if cv.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# Release the camera and close the window\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
