{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `train_model.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file processes the training data and trains the model accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import MediaPipe hands**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1738373224.957681 27107037 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1 Pro\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "\n",
    "# MediaPipe hands & extract_keypoints from main.py\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=True, \n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Extract keypoints**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This `extract_keypoints` function: \n",
    "1) Resizes each image\n",
    "2) Changes image to RGB (Red, Green, Blue) for MediaPipe because OpenCv is in BGR\n",
    "3) Gets landmarks for each image (INSERT MEDIAPIPE LANDMARK IMAGE HERE)\n",
    "4) Expected output is a list of 63 values (21 landmarks x 3 coordinates) for each image/hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "def extract_keypoints(image):\n",
    "    \"\"\" Extract hand keypoints from user using MediaPipe \"\"\"\n",
    "    try:\n",
    "        # Revise image -> better preformance\n",
    "        image = cv.resize(image, (640, 480)) # (640, 480) is default webcam resolution\n",
    "                                             # Consistent input size will help improve speed \n",
    "                                             # and stability of landmark detection\n",
    "\n",
    "        # Convert image to RGB spectrum\n",
    "        image_rgb = cv.cvtColor(image, cv.COLOR_BGR2RGB) # OpenCV images are by default BGR format\n",
    "        results = hands.process(image_rgb) # Process image with MediaPipe\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            landmarks = results.multi_hand_landmarks[0].landmark # Get hand's landmark\n",
    "                                                                 # returns a list of 21 landmark points for a single hand detected\n",
    "                                                                 # Each landmark has an x, y, and z coordinate\n",
    "            keypoints = []\n",
    "            for landmark in landmarks:\n",
    "                keypoints.append(landmark.x)  # x-coord\n",
    "                keypoints.append(landmark.y)  # Y-coord\n",
    "                keypoints.append(landmark.z)  # Z-coord (depth)\n",
    "            return keypoints\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\")\n",
    "    return None\n",
    "\n",
    "    # Expected output is a list of 63 values (21 landmarks x 3 coordinates) for an image/hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prepare data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This `prepare_data` function:\n",
    "1) Goes through each class (letter) folder and processes each of the 3000 photos\n",
    "2) Applies extract_keypoints to each image\n",
    "3) It then takes each list of 63 values (21 landmarks x 3 coordinates) for each image/hand and saves them as numpy arrays for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1738373224.983266 27143233 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def prepare_data(data_dir, num_samples=None):\n",
    "    \"\"\" Prepare the training data \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    sample_count = 0\n",
    "\n",
    "    # Iterate through each class/letter folder\n",
    "    for label in os.listdir(data_dir):\n",
    "        class_folder = os.path.join(data_dir, label)\n",
    "        if os.path.isdir(class_folder):\n",
    "            class_images = os.listdir(class_folder)\n",
    "            print(f\"Processing {len(class_images)} images for class {label}\")\n",
    "\n",
    "            # Process each image in the class folder\n",
    "            for img_name in class_images:\n",
    "                img_path = os.path.join(class_folder, img_name)\n",
    "                image = cv.imread(img_path) # Reads the image with OpenCV\n",
    "                \n",
    "                if image is not None:\n",
    "                    keypoints = extract_keypoints(image)  # MediaPipe landmarks applied to image if hand detected \n",
    "                    if keypoints:\n",
    "                        images.append(keypoints) \n",
    "                        labels.append(label)\n",
    "                        sample_count += 1\n",
    "                        \n",
    "                        # Optionally stop after 'num_samples' images for quick testing\n",
    "                        if num_samples and sample_count >= num_samples:\n",
    "                            break\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    print(f\"Total images loaded: {len(images)}\")\n",
    "\n",
    "    # Encode labels as integers\n",
    "    label_encoder = LabelEncoder() # sklearn\n",
    "    labels = label_encoder.fit_transform(labels)\n",
    "    \n",
    "    return images, labels, label_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prepare the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset, saving X, y, label_encoder for each image after processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1738373225.041209 27143233 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 3000 images for class R\n",
      "Processing 3000 images for class U\n",
      "Processing 3000 images for class I\n",
      "Processing 3000 images for class N\n",
      "Processing 3000 images for class G\n",
      "Processing 3000 images for class Z\n",
      "Processing 3000 images for class T\n",
      "Processing 3000 images for class S\n",
      "Processing 3000 images for class A\n",
      "Processing 3000 images for class F\n",
      "Processing 3000 images for class O\n",
      "Processing 3000 images for class H\n",
      "Processing 3000 images for class nothing\n",
      "Processing 3000 images for class M\n",
      "Processing 3000 images for class J\n",
      "Processing 3000 images for class C\n",
      "Processing 3000 images for class D\n",
      "Processing 3000 images for class V\n",
      "Processing 3000 images for class Q\n",
      "Processing 3000 images for class X\n",
      "Processing 3000 images for class E\n",
      "Processing 3000 images for class B\n",
      "Processing 3000 images for class K\n",
      "Processing 3000 images for class L\n",
      "Processing 3000 images for class Y\n",
      "Processing 3000 images for class P\n",
      "Processing 3000 images for class W\n",
      "Total images loaded: 56270\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data\n",
    "data_dir = '/Users/carlopisacane/Desktop/Honors/HonorsThesis/data/asl_alphabet_train'  # Path to your dataset\n",
    "X, y, label_encoder = prepare_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This model is trained by:\n",
    "1) Split the data into training and validation sets\n",
    "2) Build a machine learning model with two **Dense** layers and **Dropout** for regularization\n",
    "3) Train the model on the extracted keypoints for 10 epochs\n",
    "4) Save the trained model to a file named `asl_model.h5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 45016\n",
      "Validation samples: 11254\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Training set (X_train, y_train)\n",
    "# Validation set (X_val, y_val)  \n",
    "print(\"Training samples:\", X_train.shape[0])\n",
    "print(\"Validation samples:\", X_val.shape[0])\n",
    "\n",
    "# test_size=0.2 means 20% of data goes to validation and 80% remains for training model\n",
    "# random_state=42 ensures that the split is reproducible\n",
    "\n",
    "# You need validation because the model is always adjusting weights to minimize loss,\n",
    "# you risk overfitting the model if only track performance on data being trained.\n",
    "\n",
    "# In model.fit -> the validation data is set at the end of each epoch and reports the validation\n",
    "# loss and accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,690</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,192\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m)             │         \u001b[38;5;34m1,690\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,138</span> (70.85 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m18,138\u001b[0m (70.85 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,138</span> (70.85 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m18,138\u001b[0m (70.85 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout#, Flaten\n",
    "\n",
    "# Build the model with regularization\n",
    "model = Sequential([\n",
    "    Dense(128, input_dim=X_train.shape[1], activation='relu'), # Input layer and first dense layer\n",
    "    Dropout(0.5), # Sets 50% of inputs to zero, help prevent overfitting\n",
    "    Dense(64, activation='relu'), # Second dense layer\n",
    "    Dropout(0.5), # Another dropout layer\n",
    "    Dense(26, activation='softmax') # Softmax outputs probability distribution across all 26 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', # Adapts the learning rate for each parameter using Adam\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary() # Print model architecture summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and save it in `asl_model.h5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 890us/step - accuracy: 0.9034 - loss: 0.2901 - val_accuracy: 0.9594 - val_loss: 0.1340\n",
      "Epoch 2/10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 827us/step - accuracy: 0.9012 - loss: 0.2996 - val_accuracy: 0.9632 - val_loss: 0.1171\n",
      "Epoch 3/10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 814us/step - accuracy: 0.9057 - loss: 0.2897 - val_accuracy: 0.9662 - val_loss: 0.1178\n",
      "Epoch 4/10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 816us/step - accuracy: 0.9098 - loss: 0.2847 - val_accuracy: 0.9684 - val_loss: 0.1124\n",
      "Epoch 5/10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 740us/step - accuracy: 0.9051 - loss: 0.2912 - val_accuracy: 0.9687 - val_loss: 0.1147\n",
      "Epoch 6/10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 750us/step - accuracy: 0.9091 - loss: 0.2767 - val_accuracy: 0.9618 - val_loss: 0.1107\n",
      "Epoch 7/10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 735us/step - accuracy: 0.9081 - loss: 0.2740 - val_accuracy: 0.9653 - val_loss: 0.1228\n",
      "Epoch 8/10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 794us/step - accuracy: 0.9051 - loss: 0.2884 - val_accuracy: 0.9679 - val_loss: 0.1107\n",
      "Epoch 9/10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 768us/step - accuracy: 0.9078 - loss: 0.2761 - val_accuracy: 0.9696 - val_loss: 0.1043\n",
      "Epoch 10/10\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 841us/step - accuracy: 0.9090 - loss: 0.2804 - val_accuracy: 0.9725 - val_loss: 0.1074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as asl_model.h5\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=10, \n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "\n",
    "model.save('model/asl_model.h5')  # Save the trained model\n",
    "print(\"Model saved as asl_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
